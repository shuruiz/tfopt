{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "from network import Network\n",
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "def _create_population(nn_sturcture, size=100):\n",
    "    \"\"\"\n",
    "    size: population size, number of chromesomes \n",
    "\n",
    "    return population, which is a list of random created network graphs\n",
    "    \"\"\"\n",
    "    population = []\n",
    "    for _ in range(size):\n",
    "        nn = Network(nn_sturcture[0])\n",
    "        nn.random_initialize() #  shape \n",
    "        population.append(nn)\n",
    "    return population\n",
    "\n",
    "def encode(weights, bias):\n",
    "    \"\"\"\n",
    "    encode genes to a chromosome\n",
    "    \"\"\"\n",
    "    gene=[]\n",
    "    assert len(weights)==len(bias), \"length of weiths not equal to length of bias\"\n",
    "    n_layers = len(bias)\n",
    "    for i in range(n_layers):\n",
    "        gene.extend(weights[k].flatten())\n",
    "        gene.append(bias[i])\n",
    "    return gene \n",
    "         \n",
    "\n",
    "    \n",
    "def decode(gene,structure):\n",
    "    \"\"\"\n",
    "    decoed genes of a chromosome\n",
    "    structure is the network structure, e.g.[num_inputs, 10,20,30]\n",
    "    \"\"\"\n",
    "    Weights ={}\n",
    "    Bias={}\n",
    "    \n",
    "    gene_index = 0\n",
    "    for index in range(len(structure)):\n",
    "        weight=np.zeros((structure[index],structure[index+1]))\n",
    "        bias =[]\n",
    "        for i in range(structure[index]):\n",
    "            for j in range(structure[index+1]):\n",
    "                weight[i][j] = gene[gene_index]\n",
    "                gene_index+=1\n",
    "        bias.append(gene[gene_index])\n",
    "        gene_index+=1\n",
    "        Weights[index] = weight\n",
    "        Bias[index] = bias\n",
    "        \n",
    "    return Weights, Bias\n",
    "           \n",
    "    \n",
    "def mutate(chro, mutation_rate):\n",
    "    #for each gene, it can decide to change itself to the mean of its nearby genes or not\n",
    "    new_chro = []\n",
    "        \n",
    "    for i in range(len(chro)-1):\n",
    "        xx = (chro[i]+chro[i-1]+chro[i+1] )/3\n",
    "        new_chro.append(xx)\n",
    "    return new_chro\n",
    "    \n",
    "\n",
    "def crossover(chro1, chro2):\n",
    "    # create a random crossover point\n",
    "    assert len(chro1) ==len(chro2), \"chromosome not the same length\"\n",
    "    index = np.random.randint(low=1, high=len(chro1))\n",
    "    temp1 = chro1[index:]\n",
    "    temp3 = chro1[:index]\n",
    "    temp2 = chro2[:index]\n",
    "    temp4 = chro2[index:]\n",
    "    #random combine half of chro1 genes and half of chro2 genes below\n",
    "    rd = np.random.choice([1,2,3,4])\n",
    "    if rd ==1:\n",
    "        new_chro = temp1.extend(temp2)\n",
    "    if rd ==2:\n",
    "        new_chro = temp1.extend(temp4)\n",
    "    if rd ==3:\n",
    "        new_chro = temp3.extend(temp2)\n",
    "    if rd==4:\n",
    "        new_chro = temp3.extend(temp4)\n",
    "    return new_chro\n",
    "    \n",
    "\n",
    "def selection(parents,pop):\n",
    "    chirldren=[] #get two chirldren\n",
    "    chrome={}\n",
    "    for i in range(len(parents)):\n",
    "        parent_id = parents[i][0]\n",
    "        nn=pop[parent_id]\n",
    "        weight = nn.getWeights()\n",
    "        bias = nn.getBias()\n",
    "        gene = encode(weights, bias)\n",
    "        chrome[i] = gene\n",
    "    \n",
    "    chirldren[0] = crossover(chrome[0], chrome[1])\n",
    "    chirldren[1] = crossover(chrome[2],chrome[3])\n",
    "    \n",
    "    return chirldren\n",
    "           \n",
    "         \n",
    "def GeneticAlgorithmTrainner(X,y, nn_structure, episodes = 10000, mutation_rate = 0.1):\n",
    "    # first generation \n",
    "    pop = _create_population(nn_structure)\n",
    "    fit_score= []\n",
    "    \n",
    "    avg_fit_scores=[]\n",
    "    for i in range(len(pop)):\n",
    "        nn = pop[i]\n",
    "        score = fitness(nn,X,y)\n",
    "        fit_score.append([i,score])\n",
    "    \n",
    "    fit_score=np.array(fit_score)\n",
    "    fit_score[np.argsort(fit_score[:, 1])]\n",
    "    fit_score=np.flip(fit_score,0)\n",
    "    \n",
    "    len_pop=[]\n",
    "    \n",
    "    # select the nn with hightest 4 fitness scores\n",
    "    for epi in range(episodes):\n",
    "        len_pop.append(len(pop))\n",
    "        if len_pop[epi] ==len_pop[epi-1]:\n",
    "            #converged \n",
    "            print(\"converged\")\n",
    "            break \n",
    "        \n",
    "        avg_fit_scores.append(mean(fit_score[:,1]))\n",
    "        \n",
    "        luckyest = fit_score[:4]\n",
    "        \n",
    "        #remove two worst case\n",
    "#         worst2= fit_score[-2:]\n",
    "#         for s in worst2:\n",
    "#             del pop[s[0]]\n",
    "        \n",
    "        offspring = selection(luckyest, pop)\n",
    "        \n",
    "        for kid in offspring:\n",
    "            #mutation\n",
    "            p = np.random.random_sample()\n",
    "            if p <mutation_rate:\n",
    "                kid = mutate(kid, mutation_rate )\n",
    "            Weights, Bias = decode(kid)\n",
    "            nn = Network(nn_structure[0])\n",
    "            nn.setWeights(Weights)\n",
    "            nn.setBias(Bias)\n",
    "            pop.append(nn)\n",
    "        fit_score= []\n",
    "        #calculate new fitness score \n",
    "        for i in range(len(pop)):\n",
    "            nn = pop[i]\n",
    "            score = fitness(nn,X,y)\n",
    "            fit_score.append([i,score])\n",
    "        fit_score=np.array(fit_score)\n",
    "        fit_score[np.argsort(fit_score[:, 1])]\n",
    "        fit_score=np.flip(fit_score,0)\n",
    "\n",
    "    return best_fit_scores\n",
    "        \n",
    "def fitness(nn, X, y):\n",
    "    ## 1/ cross_entropy \n",
    "    W = nn.getWeights()  #W is a dict, W[i] = [[n_layer_nodes, n-1_layer_nodes]]\n",
    "    b = nn.getBias() # b is a dict, b[i] = [bias of layer n]\n",
    "    z=[]\n",
    "    a = []\n",
    "    probs = []\n",
    "    n_layer = len(W)\n",
    "    \n",
    "    for i in range(n_layer):\n",
    "        if i ==0: \n",
    "            z = X.dot(W[i])+b[i]\n",
    "            a = np.tanh(z)\n",
    "        elif i< n_layer-1:\n",
    "            z = a.dot(W[i])+b[i]\n",
    "            a = np.tanh(z)\n",
    "        else: # softmax\n",
    "            z = a.dot(W[i])+b[i]\n",
    "            probs = softmax(z)\n",
    "    # use softmax to calculate cross entropy below:\n",
    "    \n",
    "    m = y.shape[0]\n",
    "    log_likelihood = -np.log(probs[range(m),y])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    fit = 1/loss\n",
    "    return fit\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.75   13.52    0.8082 ...  4.378   5.31    3.    ]\n",
      " [18.85   16.17    0.9056 ...  2.843   6.2     2.    ]\n",
      " [15.03   14.77    0.8658 ...  1.933   5.439   1.    ]\n",
      " ...\n",
      " [10.93   12.8     0.839  ...  5.398   5.045   3.    ]\n",
      " [13.2    13.66    0.8883 ...  8.315   5.056   3.    ]\n",
      " [16.82   15.51    0.8786 ...  4.004   5.841   2.    ]]\n"
     ]
    }
   ],
   "source": [
    "#load data below\n",
    "\n",
    "with open('seeds_dataset.txt', 'r') as f:\n",
    "    x = f.readlines()\n",
    "data=[]\n",
    "\n",
    "for line in x:\n",
    "    temp=[]\n",
    "    temp.append(line.strip().split())\n",
    "    l = []\n",
    "    \n",
    "    for ele in temp[0]:\n",
    "        l.append(float(ele))\n",
    "    data.append(l)\n",
    "#     print(l)\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 7)\n",
      "(1, 42)\n"
     ]
    }
   ],
   "source": [
    "train_X = []\n",
    "train_y = []\n",
    "test_X=[]\n",
    "test_y=[]\n",
    "len_data = len(data)\n",
    "train_data = data[:int(len_data*0.8)]\n",
    "test_data = data[int(len_data*0.8):]\n",
    "train_X = np.array(train_data[:, [column for column in range(len(train_data[0])-1)]])\n",
    "test_X  = np.array(test_data[:, [column for column in range(len(train_data[0])-1)]])\n",
    "train_y = np.array([train_data[:,len(train_data[0])-1 ]])\n",
    "test_y = np.array([test_data[:,len(train_data[0])-1 ]])\n",
    "\n",
    "print(train_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 3\n",
    "n_examples = len(train_X)\n",
    "structure = [n_examples, 5,10,n_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "getWeights() takes 0 positional arguments but 1 was given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-e083f48fa51a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfit_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeneticAlgorithmTrainner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-82ce0ba7cdf1>\u001b[0m in \u001b[0;36mGeneticAlgorithmTrainner\u001b[0;34m(X, y, nn_structure, episodes, mutation_rate)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mfit_score\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-82ce0ba7cdf1>\u001b[0m in \u001b[0;36mfitness\u001b[0;34m(nn, X, y)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;31m## 1/ cross_entropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#W is a dict, W[i] = [[n_layer_nodes, n-1_layer_nodes]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetBias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# b is a dict, b[i] = [bias of layer n]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getWeights() takes 0 positional arguments but 1 was given"
     ]
    }
   ],
   "source": [
    "fit_data = GeneticAlgorithmTrainner(train_X,train_y,structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
