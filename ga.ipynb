{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random \n",
    "from network import Network\n",
    "from sklearn.utils.extmath import softmax\n",
    "\n",
    "def _create_population(size=100):\n",
    "    \"\"\"\n",
    "\tsize: population size, number of chromesomes \n",
    "\n",
    "\treturn population, which is a list of random created network graphs\n",
    "\t\"\"\"\n",
    "    population = []\n",
    "    for _ in range(size):\n",
    "        nn = Network()\n",
    "        nn.random_initialize() #  shape \n",
    "        population.append(nn)\n",
    "    return population\n",
    "\n",
    "def encode(weights, bias):\n",
    "    gene=[]\n",
    "    assert len(weights)==len(bias), \"length of weiths not equal to length of bias\"\n",
    "    n_layers = len(bias)\n",
    "    for i in range(n_layers):\n",
    "        gene.extend(weights[k].flatten())\n",
    "        gene.append(bias[i])\n",
    "         \n",
    "\n",
    "def decode(gene,structure):\n",
    "    \"\"\"\n",
    "    structure is the network structure, e.g.[num_inputs, 10,20,30]\n",
    "    \"\"\"\n",
    "    Weights ={}\n",
    "    Bias={}\n",
    "    \n",
    "    gene_index = 0\n",
    "    for index in range(len(structure)):\n",
    "        weight=np.zeros((structure[index],structure[index+1]))\n",
    "        bias =[]\n",
    "        for i in range(structure[index]):\n",
    "            for j in range(structure[index+1]):\n",
    "                weight[i][j] = gene[gene_index]\n",
    "                gene_index+=1\n",
    "        bias.append(gene[gene_index])\n",
    "        gene_index+=1\n",
    "        Weights[index] = weight\n",
    "        Bias[index] = bias\n",
    "        \n",
    "    return Weights, Bias\n",
    "                \n",
    "        \n",
    "         \n",
    "def GeneticAlgorithmTrainner(X,y):\n",
    "    pop = _create_population()\n",
    "    fit_score= []\n",
    "    for i in range(len(pop)):\n",
    "        nn = pop[i]\n",
    "        score = fitness(nn,X,y)\n",
    "        fit_score.append([i,score])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def fitness(nn, X, y):\n",
    "    ## 1/ cross_entropy \n",
    "    W = nn.getWeights()  #W is a dict, W[i] = [[n_layer_nodes, n-1_layer_nodes]]\n",
    "    b = nn.getBias() # b is a dict, b[i] = [bias of layer n]\n",
    "    z=[]\n",
    "    a = []\n",
    "    probs = []\n",
    "    n_layer = len(W)\n",
    "    \n",
    "    for i in range(n_layer):\n",
    "        if i ==0: \n",
    "            z = X.dot(W[i])+b[i]\n",
    "            a = np.tanh(z)\n",
    "        elif i< n_layer-1:\n",
    "            z = a.dot(W[i])+b[i]\n",
    "            a = np.tanh(z)\n",
    "        else: # softmax\n",
    "            z = a.dot(W[i])+b[i]\n",
    "            probs = softmax(z)\n",
    "    # use softmax to calculate cross entropy below:\n",
    "    \n",
    "    m = y.shape[0]\n",
    "    log_likelihood = -np.log(probs[range(m),y])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    fit = 1/loss\n",
    "    return fit\n",
    "    \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14.46   14.35    0.8818 ...  2.802   5.044   1.    ]\n",
      " [11.23   12.88    0.8511 ...  4.325   5.003   3.    ]\n",
      " [19.51   16.71    0.878  ...  2.962   6.185   2.    ]\n",
      " ...\n",
      " [12.19   13.2     0.8783 ...  3.631   4.87    3.    ]\n",
      " [14.79   14.52    0.8819 ...  2.704   5.111   1.    ]\n",
      " [14.43   14.4     0.8751 ...  3.975   5.144   1.    ]]\n"
     ]
    }
   ],
   "source": [
    "#load data below\n",
    "\n",
    "with open('seeds_dataset.txt', 'r') as f:\n",
    "    x = f.readlines()\n",
    "data=[]\n",
    "\n",
    "for line in x:\n",
    "    temp=[]\n",
    "    temp.append(line.strip().split())\n",
    "    l = []\n",
    "    \n",
    "    for ele in temp[0]:\n",
    "        l.append(float(ele))\n",
    "    data.append(l)\n",
    "#     print(l)\n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(168, 7)\n",
      "(1, 42)\n"
     ]
    }
   ],
   "source": [
    "train_X = []\n",
    "train_y = []\n",
    "test_X=[]\n",
    "test_y=[]\n",
    "len_data = len(data)\n",
    "train_data = data[:int(len_data*0.8)]\n",
    "test_data = data[int(len_data*0.8):]\n",
    "train_X = np.array(train_data[:, [column for column in range(len(train_data[0])-1)]])\n",
    "test_X  = np.array(test_data[:, [column for column in range(len(train_data[0])-1)]])\n",
    "train_y = np.array([train_data[:,len(train_data[0])-1 ]])\n",
    "test_y = np.array([test_data[:,len(train_data[0])-1 ]])\n",
    "\n",
    "print(train_X.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
